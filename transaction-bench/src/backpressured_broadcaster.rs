//! This module introduces [`BackpressuredBroadcaster`] which is custom
//! implementation of [`WorkersBroadcaster`].

use {
    async_trait::async_trait,
    log::{debug, warn},
    solana_tpu_client_next::{
        ConnectionWorkersSchedulerError,
        connection_workers_scheduler::WorkersBroadcaster,
        transaction_batch::TransactionBatch,
        workers_cache::{WorkersCache, WorkersCacheError, shutdown_worker},
    },
    std::net::SocketAddr,
};

/// [`BackpressuredBroadcaster`] distributes transactions among
/// `ConnectionWorker`s while applying backpressure.
///
/// It initially attempts to send transactions to all specified peers, similar
/// to [`NonblockingBroadcaster`]. If all attempts fail due to full channels, it
/// falls back to **blocking** transmission to the first worker in the list.
/// This mechanism ensures backpressure is applied when the cluster cannot
/// process transactions as fast as they are generated by `transaction-bench`.
///
/// Empirical results show that this approach sending to the first available
/// peerâ€”performs performs similarly to an alternative strategy where transactions are
/// sent **concurrently and blockingly** to all peers, canceling the others as
/// soon as one accepts them.
pub struct BackpressuredBroadcaster;

#[async_trait]
impl WorkersBroadcaster for BackpressuredBroadcaster {
    async fn send_to_workers(
        workers: &mut WorkersCache,
        leaders: &[SocketAddr],
        transaction_batch: TransactionBatch,
    ) -> Result<(), ConnectionWorkersSchedulerError> {
        // this part is taken from NonblockingBroadcaster with modification of
        // counting successful try_send.
        let mut num_delivered: usize = 0;
        for new_leader in leaders {
            if !workers.contains(new_leader) {
                warn!("No existing worker for {new_leader:?}, skip sending to this leader.");
                continue;
            }

            let send_res =
                workers.try_send_transactions_to_address(new_leader, transaction_batch.clone());
            match send_res {
                Ok(()) => num_delivered = num_delivered.saturating_add(1),
                Err(WorkersCacheError::ShutdownError) => {
                    debug!("Connection to {new_leader} was closed, worker cache shutdown");
                }
                Err(WorkersCacheError::ReceiverDropped) => {
                    // Remove the worker from the cache, if the peer has disconnected.
                    shutdown_worker(workers.pop(*new_leader).unwrap());
                }
                Err(err) => {
                    warn!("Failed to send batch to {new_leader}, worker error: {err}");
                    // If we have failed to send batch, it will be dropped.
                }
            }
        }

        if num_delivered != 0 {
            // If we managed to enqueue with at least one worker it is good
            // enough. See doc comment for the [`BackpressuredBroadcaster`]
            // definition.
            return Ok(());
        }

        // All the leader queues are full, so we block until we manage to
        // enqueue into at least one of them.
        for new_leader in leaders {
            if !workers.contains(new_leader) {
                continue;
            }

            let send_res = workers
                .send_transactions_to_address(new_leader, transaction_batch.clone())
                .await;
            match send_res {
                Ok(()) => {
                    // Delivery to a single leader is enough.
                    return Ok(());
                }
                Err(WorkersCacheError::ShutdownError) => {
                    debug!("Connection to {new_leader} was closed, worker cache shutdown");
                }
                Err(WorkersCacheError::ReceiverDropped) => {
                    // Remove the worker from the cache, if the peer has disconnected.
                    shutdown_worker(workers.pop(*new_leader).unwrap());
                }
                Err(_) => {
                    unreachable!("The only possible error is ReceiverDropped or ShutdownError.");
                }
            }
        }

        // Fail to enqueue due to the queues being full and/or the leaders
        // disconnecting. The client will still need to verify transaction
        // landed by subscribing to transaction events and/or checking the block
        // content. We don't return error here because errors of this method are
        // treated as critical. If tpu-client-next implement a strategy to
        // handle non-critical errors somehow, return here such that that the
        // client would know that their transaction was not queued at all, and
        // could retry faster. They still need to rely on checks later in the
        // processing chain, but a faster retry could improve latency for cases
        // when the leaders are overloaded.
        Ok(())
    }
}
